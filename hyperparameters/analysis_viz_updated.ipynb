{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###\n",
    "\n",
    "def list_files_in_directory(directory_path, extension='.csv'):\n",
    "    \"\"\"\n",
    "    Returns a list of full paths for all files in the given directory path \n",
    "    with a specific extension and forward slashes.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to list files from.\n",
    "        extension (str): The file extension to filter for (e.g., '.txt').\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full file paths in the directory with the given extension and forward slashes.\n",
    "    \"\"\"\n",
    "    # Ensure the directory path itself has forward slashes\n",
    "    directory_path = directory_path.replace('\\\\', '/')\n",
    "    \n",
    "    try:\n",
    "        # List all files with the given extension\n",
    "        files = [\n",
    "            os.path.join(directory_path, f).replace('\\\\', '/') \n",
    "            for f in os.listdir(directory_path) \n",
    "            if os.path.isfile(os.path.join(directory_path, f)) and f.endswith(extension)\n",
    "        ]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(\"The directory does not exist.\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "def create_file_dict(file_paths):\n",
    "    \"\"\"\n",
    "    Creates a dictionary where keys are derived from characters before the second underscore in the file name.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of file paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with keys based on file names and paths as values.\n",
    "    \"\"\"\n",
    "    file_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        # Extract the file name without the directory path\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Split by underscores and join the first two parts as the key\n",
    "        key = '_'.join(file_name.split('_')[:2])\n",
    "        \n",
    "        # Add to dictionary\n",
    "        file_dict[key] = file_path\n",
    "        \n",
    "    return file_dict\n",
    "\n",
    "\n",
    "def concatenate_and_save_datasets(df1, df2, save_path): \n",
    "    \"\"\"\n",
    "    This function concatenates two datasets and saves the result as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - df1, df2: The datasets to concatenate.\n",
    "    - save_path: The file path to save the resulting CSV.\n",
    "    \"\"\"\n",
    "    # Concatenating the datasets\n",
    "    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Saving the concatenated DataFrame as a CSV file\n",
    "    concatenated_df.to_csv(save_path, index=False)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "\n",
    "def find_best_lr_for_datasets(datasets_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of datasets, where keys are dataset names and values are DataFrames,\n",
    "    this function returns a dictionary where each key is a dataset name, and the value is\n",
    "    the best learning rate (lr) based on the lowest mean test loss.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets_dict: dictionary of DataFrames with dataset names as keys.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with dataset names as keys and best lr values as values.\n",
    "    \"\"\"\n",
    "    best_lrs = {}\n",
    "\n",
    "    for ds_name, dataset in datasets_dict.items():\n",
    "        # Group by 'lr' and calculate the mean and standard deviation of 'test_loss'\n",
    "        lr_summary = dataset.groupby('lr')['test_loss'].agg(['mean', 'std']).reset_index()\n",
    "        lr_summary = lr_summary.rename(columns={'mean': 'loss_mean', 'std': 'loss_std'})\n",
    "\n",
    "        # Identify the learning rate with the minimum mean test loss\n",
    "        best_lr = lr_summary.loc[lr_summary['loss_mean'].idxmin(), 'lr']\n",
    "\n",
    "        # Store in the dictionary\n",
    "        best_lrs[ds_name] = best_lr\n",
    "\n",
    "    return best_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dna_dropout': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/dna_dropout_results.csv', 'dna_l2': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/dna_l2_results.csv', 'splice_dropout': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/splice_dropout_results.csv', 'splice_l2': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/splice_l2_results.csv', 'twomoons_dropout': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/twomoons_dropout_results.csv', 'twomoons_l2': 'H:/Uni/SRP projects/Cluster-results/Results 2025/First run/twomoons_l2_results.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# data_folder = r'C:\\Users\\canel\\Downloads\\results'\n",
    "data_folder = 'H:\\\\Uni\\\\SRP projects\\\\Cluster-results\\\\Results 2025\\\\First run'\n",
    "csv_list = list_files_in_directory(data_folder)\n",
    "file_dict = create_file_dict(csv_list)\n",
    "print(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dict = {\n",
    "#     'dna_dropout': 'C:/Users/canel/Downloads/with learning rate results in csv/dna_dropout_results.csv', \n",
    "#     # 'dna_l2': 'C:/Users/canel/Downloads/with learning rate results in csv/dna_l2_results.csv', \n",
    "#     # # 'protein_dropout': 'C:/Users/canel/Downloads/with learning rate results in csv/protein_dropout_results.csv', \n",
    "#     # 'protein_l2': 'C:/Users/canel/Downloads/with learning rate results in csv/protein_l2_results.csv', \n",
    "#     # 'splice_dropout': 'C:/Users/canel/Downloads/with learning rate results in csv/splice_dropout_results.csv', \n",
    "#     # 'splice_l2': 'C:/Users/canel/Downloads/with learning rate results in csv/splice_l2_results.csv', \n",
    "#     # 'twomoons_dropout': 'C:/Users/canel/Downloads/with learning rate results in csv/twomoons_dropout_results.csv', \n",
    "#     # 'twomoons_l2': 'C:/Users/canel/Downloads/with learning rate results in csv/twomoons_l2_results.csv'\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/dna_dropout_results.csv\" read.\n",
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/dna_l2_results.csv\" read.\n",
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/splice_dropout_results.csv\" read.\n",
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/splice_l2_results.csv\" read.\n",
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/twomoons_dropout_results.csv\" read.\n",
      "Dataset in \"H:/Uni/SRP projects/Cluster-results/Results 2025/First run/twomoons_l2_results.csv\" read.\n"
     ]
    }
   ],
   "source": [
    "datasets_dict = {}\n",
    "\n",
    "for ds_name, ds_path in file_dict.items():\n",
    "    datasets_dict[ds_name] = pd.read_csv(ds_path)\n",
    "\n",
    "    if 'loss' in datasets_dict[ds_name].columns:\n",
    "        datasets_dict[ds_name].rename(columns={'loss': 'test_loss'}, inplace=True)\n",
    "\n",
    "    print(f'Dataset in \"{ds_path}\" read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_reg_by_data_size(df, x_range=None):\n",
    "    \"\"\"\n",
    "    Plots the regularization value that produced the best loss for each data size\n",
    "    at each specified learning rate.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset containing the experiment results.\n",
    "    learning_rates (list of float): A list of learning rates to filter the data.\n",
    "    x_range (tuple, optional): A tuple specifying the range for the x-axis, in the form (min, max).\n",
    "\n",
    "    Returns:\n",
    "    None: Displays a subplot of scatter plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_name = df['dataset_name'].unique()[0]\n",
    "    reg_type = df['reg_type'].unique()[0]\n",
    "    learning_rates = list(df['lr'].unique())\n",
    "    \n",
    "    # Determine number of rows required for a 2-column layout\n",
    "    num_plots = len(learning_rates)\n",
    "    num_rows = (num_plots + 1) // 2\n",
    "    \n",
    "    # Create subplot figure\n",
    "    fig = sp.make_subplots(rows=num_rows, cols=2, subplot_titles=[f'Learning Rate = {lr}' for lr in learning_rates])\n",
    "    \n",
    "    # Generate a scatter plot for each learning rate\n",
    "    for i, learning_rate in enumerate(learning_rates):\n",
    "        row = (i // 2) + 1\n",
    "        col = (i % 2) + 1\n",
    "\n",
    "        # Filter dataset by the current learning rate\n",
    "        filtered_df = df[df['lr'] == learning_rate]\n",
    "        \n",
    "        # Find the regularization value that produced the best loss for each data_size_pct\n",
    "        best_reg_values = filtered_df.loc[filtered_df.groupby('data_size_pct')['test_loss'].idxmin()]\n",
    "\n",
    "        # Create scatter plot\n",
    "        scatter = px.scatter(\n",
    "            best_reg_values,\n",
    "            x='data_size_pct',\n",
    "            y='reg_val',\n",
    "            labels={'data_size_pct': 'Data Size (%)', 'reg_val': 'Best Regularization Value'},\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        # Add the scatter plot to the subplot\n",
    "        for trace in scatter['data']:\n",
    "            fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "        # Add axis titles for each subplot and apply x-axis range if provided\n",
    "        fig.update_xaxes(title_text=\"Data Size (%)\", range=x_range, row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Best Regularization Value\", row=row, col=col)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(height=num_rows * 300, width=1000, title_text=f\"{df_name.upper()} {reg_type.upper()}. Best Regularization Value by Data Size\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'df' is your DataFrame and 'lr_list' is your list of learning rates\n",
    "# plot_best_reg_by_data_size(df, lr_list, x_range=(0, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_surface_with_avg(dataset, x_col, y_col, z_col, log_x=False, log_y=False, log_z=False, z_min=None, z_max=None):\n",
    "    \"\"\"\n",
    "    Plots a 3D surface with average values of z_col grouped by x_col and y_col.\n",
    "    Filters data by z_col range if specified.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): The dataset containing the data.\n",
    "    x_col (str): The column name for the x-axis.\n",
    "    y_col (str): The column name for the y-axis.\n",
    "    z_col (str): The column name for the z-axis.\n",
    "    log_x (bool): Set x-axis to logarithmic scale if True.\n",
    "    log_y (bool): Set y-axis to logarithmic scale if True.\n",
    "    log_z (bool): Set z-axis to logarithmic scale if True.\n",
    "    z_min (float, optional): Minimum value for filtering the z-axis data.\n",
    "    z_max (float, optional): Maximum value for filtering the z-axis data.\n",
    "\n",
    "    Returns:\n",
    "    fig: Plotly Figure object.\n",
    "    \"\"\"\n",
    "    df_name = dataset['dataset_name'].unique()[0]\n",
    "    reg_type = dataset['reg_type'].unique()[0]\n",
    "\n",
    "    # Filter data based on z_min and z_max if provided\n",
    "    if z_min is not None:\n",
    "        dataset = dataset[dataset[z_col] >= z_min]\n",
    "    if z_max is not None:\n",
    "        dataset = dataset[dataset[z_col] <= z_max]\n",
    "\n",
    "    # Group by x and y and compute the mean of the z column\n",
    "    df_grouped = dataset.groupby([x_col, y_col])[z_col].mean().reset_index()\n",
    "\n",
    "    # Pivot the grouped data to get the correct format for the surface plot\n",
    "    df_pivot = df_grouped.pivot(index=y_col, columns=x_col, values=z_col)\n",
    "\n",
    "    # Create x, y, z values for the surface plot\n",
    "    x_vals = df_pivot.columns\n",
    "    y_vals = df_pivot.index\n",
    "    z_vals = df_pivot.values\n",
    "\n",
    "    # Create a 3D surface plot with opacity set to 0.6 and contours on the z-axis\n",
    "    surface = go.Surface(\n",
    "        z=z_vals, \n",
    "        x=x_vals, \n",
    "        y=y_vals, \n",
    "        opacity=0.9, \n",
    "        colorscale='Jet',\n",
    "        contours={\n",
    "            \"z\": {\n",
    "                \"show\": True,\n",
    "                \"color\": \"black\",\n",
    "                \"width\": 2\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add a 3D scatter plot with the same data points\n",
    "    scatter_data = go.Scatter3d(\n",
    "        x=df_grouped[x_col], \n",
    "        y=df_grouped[y_col], \n",
    "        z=df_grouped[z_col],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=df_grouped[z_col], colorscale='Jet', opacity=0.9)\n",
    "    )\n",
    "\n",
    "    # Identify the row with the minimum z_col value\n",
    "    min_row = df_grouped.loc[df_grouped[z_col].idxmin()]\n",
    "\n",
    "    # Create a separate scatter trace for the minimum loss point with a red marker\n",
    "    min_scatter = go.Scatter3d(\n",
    "        x=[min_row[x_col]],\n",
    "        y=[min_row[y_col]],\n",
    "        z=[min_row[z_col]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='red')\n",
    "    )\n",
    "\n",
    "    # Create the figure and add the surface, scatter plot, and min scatter plot\n",
    "    fig = go.Figure(data=[surface, scatter_data, min_scatter])\n",
    "\n",
    "    # Update axis types to logarithmic if requested\n",
    "    axis_settings = dict(\n",
    "        xaxis=dict(title=x_col, type='log' if log_x else 'linear'),\n",
    "        yaxis=dict(title=y_col, type='log' if log_y else 'linear'),\n",
    "        zaxis=dict(title=z_col, type='log' if log_z else 'linear')\n",
    "    )\n",
    "\n",
    "    # Update the layout for better visualization\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        scene=dict(\n",
    "            aspectratio=dict(x=1.25, y=1, z=1),\n",
    "            xaxis=axis_settings['xaxis'],\n",
    "            yaxis=axis_settings['yaxis'],\n",
    "            zaxis=axis_settings['zaxis']\n",
    "        ),\n",
    "        margin=dict(l=0.5, r=0.5, b=50, t=50),\n",
    "        title=f'{df_name.upper()}. {reg_type.upper()}. {z_col}' \n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "# Example usage with filtering on z-axis values\n",
    "# plot_3d_surface_with_avg(dataset, 'x_col_name', 'y_col_name', 'z_col_name', z_min=0, z_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_with_normal_fit(\n",
    "    dataset,\n",
    "    reg_val='all',\n",
    "    data_size_pct='all',\n",
    "    opacity=0.6,\n",
    "    line_color='blue'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots histograms of 'loss' values and fits them to normal distribution curves.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): DataFrame containing at least ['reg_val', 'data_size_pct', 'loss'] columns.\n",
    "    - reg_val (float, str, or list, optional): Specific regularization value(s) to include or 'all'. Default is 'all'.\n",
    "    - data_size_pct (float, str, or list, optional): Specific data size percentage(s) to include or 'all'. Default is 'all'.\n",
    "    - opacity (float, optional): Opacity level for the histogram bars (0 to 1). Default is 0.6.\n",
    "    - line_color (str, optional): Color for the histogram bars and normal distribution lines. Default is 'blue'.\n",
    "    \n",
    "    The function generates subplots using Plotly, with each subplot representing a histogram\n",
    "    of 'loss' values for the specified (reg_val, data_size_pct) combinations. Each histogram\n",
    "    is overlaid with a fitted normal distribution curve and annotated with the mean and standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input DataFrame\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise TypeError(\"dataset must be a pandas DataFrame.\")\n",
    "    \n",
    "    required_columns = {'reg_val', 'data_size_pct', 'loss'}\n",
    "    if not required_columns.issubset(dataset.columns):\n",
    "        raise ValueError(f\"dataset must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Handle 'all' or specific reg_val\n",
    "    if reg_val == 'all':\n",
    "        reg_vals = dataset['reg_val'].unique()\n",
    "    elif isinstance(reg_val, list):\n",
    "        reg_vals = reg_val\n",
    "    else:\n",
    "        reg_vals = [reg_val]\n",
    "    \n",
    "    # Handle 'all' or specific data_size_pct\n",
    "    if data_size_pct == 'all':\n",
    "        data_size_pcts = dataset['data_size_pct'].unique()\n",
    "    elif isinstance(data_size_pct, list):\n",
    "        data_size_pcts = data_size_pct\n",
    "    else:\n",
    "        data_size_pcts = [data_size_pct]\n",
    "    \n",
    "    # Generate all valid combinations\n",
    "    combinations = []\n",
    "    for rv in reg_vals:\n",
    "        for dscp in data_size_pcts:\n",
    "            if not dataset[(dataset['reg_val'] == rv) & (dataset['data_size_pct'] == dscp)].empty:\n",
    "                combinations.append((rv, dscp))\n",
    "    \n",
    "    if not combinations:\n",
    "        raise ValueError(\"No valid (reg_val, data_size_pct) combinations found for plotting.\")\n",
    "    \n",
    "    # Determine subplot layout (n rows x 2 columns)\n",
    "    n = len(combinations)\n",
    "    n_cols = 2\n",
    "    n_rows = (n + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Create subplot titles\n",
    "    subplot_titles = [f\"reg_val={rv}, data_size_pct={dscp}\" for rv, dscp in combinations]\n",
    "    \n",
    "    # Initialize subplots\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles)\n",
    "    \n",
    "    # Compute global x-axis range based on all data\n",
    "    global_x_min = dataset['loss'].min()\n",
    "    global_x_max = dataset['loss'].max()\n",
    "    \n",
    "    # Increase the x range by 10% of the range\n",
    "    x_range = global_x_max - global_x_min\n",
    "    x_padding = 0.10 * x_range\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    # Iterate over each combination and plot\n",
    "    for idx, (rv, dscp) in enumerate(combinations):\n",
    "        row = (idx) // n_cols + 1\n",
    "        col = (idx) % n_cols + 1\n",
    "        \n",
    "        # Extract 'loss' data for the current combination\n",
    "        subset = dataset[(dataset['reg_val'] == rv) & (dataset['data_size_pct'] == dscp)]['loss']\n",
    "        \n",
    "        if subset.empty:\n",
    "            continue  # Skip if no data\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean = subset.mean()\n",
    "        stdv = subset.std()\n",
    "        \n",
    "        # Determine optimal number of bins using Freedman-Diaconis rule\n",
    "        q25, q75 = np.percentile(subset, [25, 75])\n",
    "        iqr = q75 - q25\n",
    "        bin_width = 2 * iqr * len(subset) ** (-1/3)\n",
    "        if bin_width > 0:\n",
    "            bins = int(np.ceil((global_x_max - global_x_min) / bin_width))\n",
    "            bins = min(100, bins)\n",
    "        else:\n",
    "            bins = 30  # Fallback to default if IQR is zero\n",
    "            \n",
    "        \n",
    "        # Generate histogram\n",
    "        hist = go.Histogram(\n",
    "            x=subset,\n",
    "            nbinsx=bins,\n",
    "            name='Loss Histogram',\n",
    "            marker_color=line_color,\n",
    "            opacity=opacity,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(hist, row=row, col=col)\n",
    "        \n",
    "        # Generate normal distribution line\n",
    "        x = np.linspace(global_x_min, global_x_max, 500)\n",
    "        y = norm.pdf(x, mean, stdv) * len(subset) * (global_x_max - global_x_min) / bins  # Scale to histogram\n",
    "        norm_line = go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2),\n",
    "            name='Normal Fit',\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(norm_line, row=row, col=col)\n",
    "        \n",
    "        # Calculate maximum count in histogram to position annotation\n",
    "        hist_counts = subset.value_counts(bins=bins)\n",
    "        y_max_hist = hist_counts.max()\n",
    "        \n",
    "        # Compute annotation position near the top right corner\n",
    "        annotation_x = global_x_max - 0.05 * x_range\n",
    "        annotation_y = y_max_hist * 1.05\n",
    "        \n",
    "        # Add annotation for mean and standard deviation in the top right corner\n",
    "        annotation_text = f\"Mean: {mean:.3f}<br>Std Dev: {stdv:.3f}\"\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=annotation_x,\n",
    "            y=annotation_y,\n",
    "            xref=f\"x{idx+1}\",\n",
    "            yref=f\"y{idx+1}\",\n",
    "            text=annotation_text,\n",
    "            showarrow=False,\n",
    "            xanchor='right',\n",
    "            yanchor='bottom',\n",
    "            font=dict(color='black', size=12)\n",
    "        )\n",
    "        \n",
    "        # Update y-axis for the current subplot\n",
    "        fig.update_yaxes(title_text=\"Count\", row=row, col=col)\n",
    "    \n",
    "    # Update x-axis range for all subplots to be the same\n",
    "    for i in range(1, n + 1):\n",
    "        row = (i - 1) // n_cols + 1\n",
    "        col = (i - 1) % n_cols + 1\n",
    "        fig.update_xaxes(range=[global_x_min, global_x_max], row=row, col=col)\n",
    "    \n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        height=400 * n_rows,  # Adjust height based on number of rows\n",
    "        width=900,             # Set a reasonable width\n",
    "        showlegend=False,     # Hide legend\n",
    "        title_text=f\"{dataset_name.upper()}.Histograms of Loss with Normal Distribution Fits\",\n",
    "        margin=dict(t=80, b=50, l=50, r=50)  # Adjust margins\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots_to_html(plot_list, file_name):\n",
    "    \"\"\"\n",
    "    Saves multiple Plotly plots into a single HTML file.\n",
    "\n",
    "    Parameters:\n",
    "    - plot_list: A list of Plotly Figure objects.\n",
    "    - file_name: The name of the HTML file to save the plots.\n",
    "    \"\"\"\n",
    "    import plotly.io as pio\n",
    "\n",
    "    # Initialize an empty list to store the HTML content of each plot\n",
    "    html_content = []\n",
    "\n",
    "    # Loop through the list of plot objects and convert each to HTML\n",
    "    for plot in plot_list:\n",
    "        html_content.append(pio.to_html(plot, full_html=False))\n",
    "\n",
    "    # Concatenate all plots into a single HTML string\n",
    "    full_html = \"<html><head></head><body>\" + \"\".join(html_content) + \"</body></html>\"\n",
    "\n",
    "    # Save the HTML content to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(full_html)\n",
    "\n",
    "    print(f\"Plots saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_metric(df, x_axis, y_axis, group_by, log_x=False, auto_scale=False, message=''):\n",
    "    \"\"\"\n",
    "    Plots the distribution of loss as a box plot for each unique group_by value using Plotly subplots.\n",
    "    Each subplot shows the box plot distribution for values of the x_axis.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - x_axis: column name to use for the x-axis ('data_size_pct' or 'reg_val').\n",
    "    - group_by: column name to group the data by ('reg_val' or 'data_size_pct').\n",
    "    - log_x: boolean indicating whether to use a log scale for the x-axis.\n",
    "    - auto_scale: boolean indicating whether to automatically adjust plot scales.\n",
    "    - same_y: boolean indicating whether all subplots should share the same y-axis range.\n",
    "    - message: additional message to include in the plot title.\n",
    "\n",
    "    Returns:\n",
    "    - Plotly Figure object.\n",
    "    \"\"\"\n",
    "    if auto_scale == False:\n",
    "        same_y = True\n",
    "    else:\n",
    "        same_y = False\n",
    "\n",
    "    df_name = df['dataset_name'].unique()[0]\n",
    "    reg_type = df['reg_type'].unique()[0]\n",
    "\n",
    "    # Validate inputs\n",
    "    if x_axis not in df.columns:\n",
    "        raise ValueError(f\"x_axis '{x_axis}' not found in DataFrame columns.\")\n",
    "    if group_by not in df.columns:\n",
    "        raise ValueError(f\"group_by '{group_by}' not found in DataFrame columns.\")\n",
    "    if x_axis == group_by:\n",
    "        raise ValueError(\"x_axis and group_by must be different columns.\")\n",
    "\n",
    "    # Make a copy of the DataFrame to avoid modifying the original data\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # Adjust x_axis to be positive by shifting if log_x is True\n",
    "    shift_value = 0\n",
    "    if log_x:\n",
    "        min_x = df_plot[x_axis].min()\n",
    "        if min_x <= 0:\n",
    "            shift_value = abs(min_x) + 1e-6  # Small constant to ensure positivity\n",
    "            df_plot[x_axis] = df_plot[x_axis] + shift_value\n",
    "            print(f\"Shifted '{x_axis}' by {shift_value} to make all values positive for log scale.\")\n",
    "\n",
    "    # Get unique groups\n",
    "    unique_groups = sorted(df_plot[group_by].unique())\n",
    "\n",
    "    # Determine number of subplots (n rows x 2 columns)\n",
    "    n = len(unique_groups)\n",
    "    cols = 2\n",
    "    rows = math.ceil(n / cols)\n",
    "\n",
    "    # Create subplot titles\n",
    "    subplot_titles = [f\"{group_by} = {g}\" for g in unique_groups]\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)\n",
    "\n",
    "    # If same_y is True, determine the global y-axis range\n",
    "    if same_y:\n",
    "        y_min = df_plot[y_axis].min()\n",
    "        y_max = df_plot[y_axis].max()\n",
    "        # Optionally, add some padding\n",
    "        y_padding = (y_max - y_min) * 0.05\n",
    "        y_range = [y_min - y_padding, y_max + y_padding]\n",
    "\n",
    "    for i, group in enumerate(unique_groups):\n",
    "        row = i // cols + 1\n",
    "        col = i % cols + 1\n",
    "        df_group = df_plot[df_plot[group_by] == group]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=df_group[x_axis],\n",
    "                y=df_group[y_axis],\n",
    "                name=str(group),\n",
    "                boxpoints='all',  # Show all points\n",
    "                jitter=0,       # Jitter for visibility\n",
    "                pointpos=0     # Offset for points\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "\n",
    "        # Set x-axis to log if needed\n",
    "        fig.update_xaxes(type='log' if log_x else 'linear', row=row, col=col)\n",
    "\n",
    "        # Set y-axis range if same_y is True\n",
    "        if same_y:\n",
    "            fig.update_yaxes(range=y_range, title_text=f\"{y_axis}\", row=row, col=col)\n",
    "        else:\n",
    "            fig.update_yaxes(title_text=f\"{y_axis}\", row=row, col=col)\n",
    "\n",
    "        # Set x-axis title\n",
    "        fig.update_xaxes(title_text=x_axis, row=row, col=col)\n",
    "\n",
    "    # If there are empty subplots, remove their annotations\n",
    "    total_subplots = rows * cols\n",
    "    existing_annotations = len(fig.layout.annotations)\n",
    "    if n < total_subplots:\n",
    "        for i in range(n, total_subplots):\n",
    "            if i < existing_annotations:\n",
    "                fig.layout.annotations[i].text = ''\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400 * rows,\n",
    "        width=1200,\n",
    "        title_text=f\"{message} {df_name.upper()} {reg_type.upper()}. {y_axis} Distribution vs {x_axis} grouped by {group_by}\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    # fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_metric(df, x_axis, y_axis, group_by, log_x=False, auto_scale=False, show_error_bars=True, message=''):\n",
    "    \"\"\"\n",
    "    Plots the mean loss as a function of x_axis for each unique group_by value using a single Plotly plot.\n",
    "    Each marker in the scatter plot can be accompanied by an error bar representing the spread of the data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - x_axis: column name to use for the x-axis ('data_size_pct' or 'reg_val').\n",
    "    - group_by: column name to group the data by ('reg_val' or 'data_size_pct').\n",
    "    - log_x: boolean indicating whether to use a log scale for the x-axis.\n",
    "    - auto_scale: boolean indicating whether to automatically adjust plot scales.\n",
    "    - show_error_bars: boolean indicating whether to display error bars representing the standard deviation.\n",
    "    - message: additional message to include in the plot title.\n",
    "\n",
    "    Returns:\n",
    "    - Plotly Figure object.\n",
    "    \"\"\"\n",
    "    # Extract dataset and regression type names\n",
    "    df_name = df['dataset_name'].unique()[0]\n",
    "    reg_type = df['reg_type'].unique()[0]\n",
    "\n",
    "    # Validate inputs\n",
    "    if x_axis not in df.columns:\n",
    "        raise ValueError(f\"x_axis '{x_axis}' not found in DataFrame columns.\")\n",
    "    if group_by not in df.columns:\n",
    "        raise ValueError(f\"group_by '{group_by}' not found in DataFrame columns.\")\n",
    "    if x_axis == group_by:\n",
    "        raise ValueError(\"x_axis and group_by must be different columns.\")\n",
    "\n",
    "    # Make a copy of the DataFrame to avoid modifying the original data\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # Adjust x_axis to be positive by shifting if log_x is True\n",
    "    shift_value = 0  # Initialize shift value\n",
    "    if log_x:\n",
    "        min_x = df_plot[x_axis].min()\n",
    "        if min_x <= 0:\n",
    "            shift_value = abs(min_x) + 1e-6  # Small constant to ensure positivity\n",
    "            df_plot[x_axis] = df_plot[x_axis] + shift_value\n",
    "            print(f\"Shifted '{x_axis}' by {shift_value} to make all values positive for log scale.\")\n",
    "\n",
    "    # Compute mean and standard deviation of loss grouped by 'group_by' and 'x_axis'\n",
    "    grouped = df_plot.groupby([group_by, x_axis])[y_axis].agg(['mean', 'std']).reset_index()\n",
    "    grouped = grouped.rename(columns={'mean': f'{y_axis}_mean', 'std': f'{y_axis}_std'})\n",
    "\n",
    "    # Get unique groups\n",
    "    unique_groups = sorted(grouped[group_by].unique())\n",
    "\n",
    "    # Create a single plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for group in unique_groups:\n",
    "        df_group = grouped[grouped[group_by] == group].sort_values(by=x_axis)\n",
    "        \n",
    "        # Prepare trace arguments\n",
    "        trace_args = {\n",
    "            'x': df_group[x_axis],\n",
    "            'y': df_group[f'{y_axis}_mean'],\n",
    "            'mode': 'lines+markers',\n",
    "            'name': f\"{group_by} = {group}\",\n",
    "        }\n",
    "\n",
    "        # Conditionally add error bars\n",
    "        if show_error_bars:\n",
    "            trace_args['error_y'] = dict(\n",
    "                type='data',\n",
    "                array=df_group[f'{y_axis}_std'],\n",
    "                visible=True\n",
    "            )\n",
    "\n",
    "        # Add the trace to the figure\n",
    "        fig.add_trace(go.Scatter(**trace_args))\n",
    "\n",
    "    # Set x-axis to log if needed\n",
    "    fig.update_xaxes(type='log' if log_x else 'linear', title_text=x_axis)\n",
    "\n",
    "    # Set y-axis title\n",
    "    fig.update_yaxes(title_text=F\"Mean {y_axis}\")\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        title_text=f\"{message} {df_name.upper()} {reg_type.upper()}. Mean {y_axis} vs {x_axis} grouped by {group_by}\",\n",
    "        legend_title=group_by\n",
    "    )\n",
    "\n",
    "    # fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_mean_metric(df, x_axis, y_axis,  group_by, log_x=False, auto_scale=True, message=''):\n",
    "    \"\"\"\n",
    "    Plots the mean loss as a function of x_axis for each unique group_by value using Plotly subplots.\n",
    "    Each marker in the scatter plot is accompanied by an error bar representing the spread of the data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - x_axis: column name to use for the x-axis ('data_size_pct' or 'reg_val').\n",
    "    - group_by: column name to group the data by ('reg_val' or 'data_size_pct').\n",
    "    - log_x: boolean indicating whether to use a log scale for the x-axis.\n",
    "    - auto_scale: boolean indicating whether to automatically adjust plot scales.\n",
    "    - same_y: boolean indicating whether all subplots should share the same y-axis range.\n",
    "    - message: additional message to include in the plot title.\n",
    "\n",
    "    Returns:\n",
    "    - Plotly Figure object.\n",
    "    \"\"\"\n",
    "    if auto_scale == True:\n",
    "        same_y = False\n",
    "    else:\n",
    "        same_y = True\n",
    "\n",
    "    # Extract dataset and regression type names\n",
    "    df_name = df['dataset_name'].unique()[0]\n",
    "    reg_type = df['reg_type'].unique()[0]\n",
    "\n",
    "    # Validate inputs\n",
    "    if x_axis not in df.columns:\n",
    "        raise ValueError(f\"x_axis '{x_axis}' not found in DataFrame columns.\")\n",
    "    if group_by not in df.columns:\n",
    "        raise ValueError(f\"group_by '{group_by}' not found in DataFrame columns.\")\n",
    "    if x_axis == group_by:\n",
    "        raise ValueError(\"x_axis and group_by must be different columns.\")\n",
    "\n",
    "    # Make a copy of the DataFrame to avoid modifying the original data\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # Adjust x_axis to be positive by shifting if log_x is True\n",
    "    shift_value = 0  # Initialize shift value\n",
    "    if log_x:\n",
    "        min_x = df_plot[x_axis].min()\n",
    "        if min_x <= 0:\n",
    "            shift_value = abs(min_x) + 1e-6  # Small constant to ensure positivity\n",
    "            df_plot[x_axis] = df_plot[x_axis] + shift_value\n",
    "            print(f\"Shifted '{x_axis}' by {shift_value} to make all values positive for log scale.\")\n",
    "\n",
    "    # Compute mean and standard deviation of loss grouped by 'group_by' and 'x_axis'\n",
    "    grouped = df_plot.groupby([group_by, x_axis])[y_axis].agg(['mean', 'std']).reset_index()\n",
    "    grouped = grouped.rename(columns={'mean': f'{y_axis}_mean', 'std': f'{y_axis}_std'})\n",
    "\n",
    "    # Get unique groups\n",
    "    unique_groups = sorted(grouped[group_by].unique())\n",
    "\n",
    "    # Determine number of subplots (n rows x 2 columns)\n",
    "    n = len(unique_groups)\n",
    "    cols = 2\n",
    "    rows = math.ceil(n / cols)\n",
    "\n",
    "    # Create subplot titles\n",
    "    subplot_titles = [f\"{group_by} = {g}\" for g in unique_groups]\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)\n",
    "\n",
    "    # If same_y is True, determine the global y-axis range\n",
    "    if same_y:\n",
    "        y_min = grouped[f'{y_axis}_mean'].min()\n",
    "        y_max = grouped[f'{y_axis}_mean'].max()\n",
    "        # Optionally, add some padding\n",
    "        y_padding = (y_max - y_min) * 0.05 if y_max != y_min else 1\n",
    "        y_range = [y_min - y_padding, y_max + y_padding]\n",
    "\n",
    "    for i, group in enumerate(unique_groups):\n",
    "        row = i // cols + 1\n",
    "        col = i % cols + 1\n",
    "        df_group = grouped[grouped[group_by] == group].sort_values(by=x_axis)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_group[x_axis],\n",
    "                y=df_group[f'{y_axis}_mean'],\n",
    "                mode='lines+markers',\n",
    "                name=str(group),\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=df_group[f'{y_axis}_std'],\n",
    "                    visible=True\n",
    "                )\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "\n",
    "        # Set x-axis to log if needed\n",
    "        fig.update_xaxes(type='log' if log_x else 'linear', autorange=True, row=row, col=col)\n",
    "\n",
    "        # Set y-axis range based on same_y parameter\n",
    "        if same_y:\n",
    "            fig.update_yaxes(range=y_range, title_text=f\"Mean {y_axis}\", row=row, col=col)\n",
    "        else:\n",
    "            fig.update_yaxes(autorange=True, title_text=f\"Mean {y_axis}\", row=row, col=col)\n",
    "\n",
    "        # Adjust x-axis title if shifted\n",
    "        if log_x and shift_value != 0:\n",
    "            fig.update_xaxes(title_text=f\"{x_axis}\", row=row, col=col)\n",
    "        else:\n",
    "            fig.update_xaxes(title_text=x_axis, row=row, col=col)\n",
    "\n",
    "    # If there are empty subplots, remove their annotations\n",
    "    total_subplots = rows * cols\n",
    "    existing_annotations = len(fig.layout.annotations)\n",
    "    if n < total_subplots:\n",
    "        for i in range(n, total_subplots):\n",
    "            if i < existing_annotations:\n",
    "                fig.layout.annotations[i].text = ''\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=320 * rows,\n",
    "        width=1000,\n",
    "        title_text=f\"{message} {df_name.upper()} {reg_type.upper()}. Mean {y_axis} vs {x_axis} grouped by {group_by}\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    # fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\DNA_DROPOUT.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\DNA_DROPOUT.html\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\DNA_L2.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\DNA_L2.html\n",
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\SPLICE_DROPOUT.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\SPLICE_DROPOUT.html\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\SPLICE_L2.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\SPLICE_L2.html\n",
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\TWOMOONS_DROPOUT.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\TWOMOONS_DROPOUT.html\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "Shifted 'reg_val' by 1e-06 to make all values positive for log scale.\n",
      "H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\TWOMOONS_L2.html\n",
      "Plots saved to H:\\Uni\\SRP projects\\Cluster-results\\Results 2025\\First run\\TWOMOONS_L2.html\n"
     ]
    }
   ],
   "source": [
    "best_lrs = find_best_lr_for_datasets(datasets_dict)\n",
    "best_lrs\n",
    "\n",
    "\n",
    "for dataset_name, dataset in datasets_dict.items():\n",
    "    plots_list = []\n",
    "\n",
    "    data_sizes = datasets_dict[dataset_name]['data_size_pct'].unique()\n",
    "\n",
    "    best_lr = best_lrs[dataset_name]\n",
    "    dataset2 = dataset[dataset['lr'] == best_lr]\n",
    "    reg_vals = dataset['reg_val'].unique()[::2]\n",
    "    dataset2 = dataset[dataset['reg_val'].isin(reg_vals)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if dataset['reg_type'].unique().item() == 'l2':\n",
    "        reg_type = 'l2'\n",
    "        log_scale = True\n",
    "\n",
    "\n",
    "    else:\n",
    "        reg_type = 'dropout'\n",
    "        log_scale = False\n",
    "        dataset2 = dataset\n",
    "\n",
    "    plot1 = subplot_mean_metric(dataset2, x_axis='reg_val', y_axis='test_loss', group_by='data_size_pct', log_x=log_scale, auto_scale=True, message='')\n",
    "    plots_list.append(plot1)\n",
    "\n",
    "\n",
    "    for datasize in data_sizes:\n",
    "\n",
    "        dataset3 =  dataset[dataset['data_size_pct'] == datasize]\n",
    "\n",
    "        # dataset3 = dataset[\n",
    "        #         (dataset['data_size_pct'] == datasize) &\n",
    "        #         (dataset['lr'] != 0.1)]\n",
    "\n",
    "        if dataset['reg_type'].unique().item() == 'l2':\n",
    "            reg_type = 'l2'\n",
    "            log_scale = True\n",
    "\n",
    "            reg_vals = dataset['reg_val'].unique()[::2]\n",
    "            dataset3 = dataset3[dataset3['reg_val'].isin(reg_vals)]\n",
    "\n",
    "        else:\n",
    "            reg_type = 'dropout'\n",
    "            log_scale = False\n",
    "            \n",
    "        if not dataset3.empty:\n",
    "            plot2 = box_plot_metric(dataset3, x_axis='lr', y_axis='test_loss', group_by='reg_val', log_x=True, auto_scale=False, message=str(datasize)+'%')\n",
    "            plots_list.append(plot2)\n",
    "        else:\n",
    "            print(f\"No data available for data_size_pct = {datasize} in {dataset_name}\")\n",
    "\n",
    "\n",
    "    for datasize in data_sizes:\n",
    "\n",
    "        dataset3 =  dataset[dataset['data_size_pct'] == datasize]\n",
    "        # dataset3 = dataset[\n",
    "        #         (dataset['data_size_pct'] == datasize) &\n",
    "        #         (dataset['lr'] != 0.1)]\n",
    "\n",
    "        if dataset['reg_type'].unique().item() == 'l2':\n",
    "            reg_type = 'l2'\n",
    "            log_scale = True\n",
    "\n",
    "            reg_vals = dataset['reg_val'].unique()[::2]\n",
    "            dataset3 = dataset3[dataset3['reg_val'].isin(reg_vals)]\n",
    "\n",
    "        else:\n",
    "            reg_type = 'dropout'\n",
    "            log_scale = False\n",
    "\n",
    "\n",
    "        if not dataset3.empty:\n",
    "            plot3 = plot_mean_metric(dataset3, x_axis='reg_val', y_axis='test_loss', group_by='lr', log_x=log_scale, auto_scale=False, show_error_bars=True, message=str(datasize)+'%')\n",
    "            plots_list.append(plot3)\n",
    "        else:\n",
    "            print(f\"No data available for data_size_pct = {datasize} in {dataset_name}\")\n",
    "\n",
    "    \n",
    "    html_filename = os.path.join(data_folder, f'{dataset_name.upper()}.html')\n",
    "    print(html_filename)\n",
    "\n",
    "    save_plots_to_html(plots_list, html_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 0 == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(datasets_dict))\n",
    "print(first_key)  # Output: 'key1'\n",
    "unique_values = list(datasets_dict[first_key]['data_size_pct'].unique())\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_non_significant_intervals(non_significant_pairs, n=1):\n",
    "    # Create an undirected graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph based on non-significant pairs\n",
    "    for _, row in non_significant_pairs.iterrows():\n",
    "        G.add_edge(row['A'], row['B'])\n",
    "    \n",
    "    # Find all connected components and sort them by size (descending order)\n",
    "    sorted_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    \n",
    "    # Get the n largest components\n",
    "    largest_intervals = [sorted(component) for component in sorted_components[:n]]\n",
    "    \n",
    "    return largest_intervals\n",
    "\n",
    "values = []\n",
    "values_dict = {}\n",
    "\n",
    "for i in unique_values:\n",
    "\n",
    "    df = datasets_dict[first_key][datasets_dict[first_key]['data_size_pct'] == i]\n",
    "\n",
    "    welch_anova = pg.welch_anova(dv='test_loss', between='reg_val', data=df)\n",
    "    # print(\"Welch’s ANOVA results:\")\n",
    "    # print(welch_anova)\n",
    "\n",
    "    # Check if there's an overall significant difference\n",
    "    if welch_anova['p-unc'][0] < 0.05:\n",
    "        # print(f\"Significant differences detected by Welch’s ANOVA. P-value: {welch_anova['p-unc'][0]} < 0.05.\")\n",
    "        \n",
    "        # Step 2: Perform Games-Howell Test for pairwise comparisons\n",
    "        games_howell = pg.pairwise_gameshowell(dv='test_loss', between='reg_val', data=df)\n",
    "        # print(\"\\nGames-Howell pairwise comparison results:\")\n",
    "        # print(games_howell)\n",
    "        \n",
    "        # Step 3: Filter out non-significant pairs (p-value >= 0.05)\n",
    "        non_significant_pairs = games_howell[games_howell['pval'] >= 0.05]\n",
    "        # print(\"\\nNon-significant pairs (test_loss is not statistically different):\")\n",
    "        # print(non_significant_pairs[['A', 'B', 'pval']])\n",
    "\n",
    "        # Sample data based on the given example\n",
    "        data = non_significant_pairs[['A', 'B', 'pval']]\n",
    "        non_significant_pairs = pd.DataFrame(data)\n",
    "\n",
    "        # Find the largest interval\n",
    "        largest_interval = largest_non_significant_intervals(non_significant_pairs)\n",
    "        print(f'Size: {i}%. Largest interval of values that are not statistically different: {largest_interval}')#. Max non-significant value: {max(largest_interval)}')\n",
    "        values_dict[i] = max(largest_interval)\n",
    "        # values.append(max(largest_interval))\n",
    "        # print(f'Size: {i}%. {max(largest_interval)}')\n",
    "\n",
    "    else:\n",
    "        print(f\"Size: {i}%. No overall significant difference found by Welch’s ANOVA. No need for pairwise comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(x=values_dict.keys(), y=values_dict.values(), labels={'x': 'X Values', 'y': 'Y Values'})\n",
    "fig.update_layout(title='Scatter Plot of X vs. Y',\n",
    "                  height=400,\n",
    "                  width=600)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "# Assuming `df` is your DataFrame with 'reg_val' and 'loss' columns\n",
    "# Step 1: Perform Welch’s ANOVA to check for overall differences\n",
    "\n",
    "df = datasets_dict['dna_l2'][datasets_dict['dna_l2']['data_size_pct'] == 4]\n",
    "\n",
    "welch_anova = pg.welch_anova(dv='test_loss', between='reg_val', data=df)\n",
    "print(\"Welch’s ANOVA results:\")\n",
    "print(welch_anova)\n",
    "\n",
    "# Check if there's an overall significant difference\n",
    "if welch_anova['p-unc'][0] < 0.05:\n",
    "    print(f\"Significant differences detected by Welch’s ANOVA. P-value: {welch_anova['p-unc'][0]} < 0.05.\")\n",
    "    \n",
    "    # Step 2: Perform Games-Howell Test for pairwise comparisons\n",
    "    games_howell = pg.pairwise_gameshowell(dv='test_loss', between='reg_val', data=df)\n",
    "    # print(\"\\nGames-Howell pairwise comparison results:\")\n",
    "    # print(games_howell)\n",
    "    \n",
    "    # Step 3: Filter out non-significant pairs (p-value >= 0.05)\n",
    "    non_significant_pairs = games_howell[games_howell['pval'] >= 0.05]\n",
    "    print(\"\\nNon-significant pairs (test_loss is not statistically different):\")\n",
    "    print(non_significant_pairs[['A', 'B', 'pval']])\n",
    "else:\n",
    "    print(\"No overall significant difference found by Welch’s ANOVA. No need for pairwise comparisons.\")\n",
    "\n",
    "\n",
    "def largest_non_significant_interval(non_significant_pairs):\n",
    "    # Create an undirected graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph based on non-significant pairs\n",
    "    for _, row in non_significant_pairs.iterrows():\n",
    "        G.add_edge(row['A'], row['B'])\n",
    "    \n",
    "    # Find the largest connected component\n",
    "    largest_component = max(nx.connected_components(G), key=len)\n",
    "    \n",
    "    # Sort the values in the largest component to get the interval\n",
    "    largest_interval = sorted(largest_component)\n",
    "    \n",
    "    return largest_interval\n",
    "\n",
    "# Sample data based on the given example\n",
    "data = non_significant_pairs[['A', 'B', 'pval']]\n",
    "non_significant_pairs = pd.DataFrame(data)\n",
    "\n",
    "# Find the largest interval\n",
    "largest_interval = largest_non_significant_interval(non_significant_pairs)\n",
    "print(\"Largest interval of values that are not statistically different:\", largest_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def largest_non_significant_interval(non_significant_pairs):\n",
    "    # Create an undirected graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph based on non-significant pairs\n",
    "    for _, row in non_significant_pairs.iterrows():\n",
    "        G.add_edge(row['A'], row['B'])\n",
    "    \n",
    "    # Find the largest connected component\n",
    "    largest_component = max(nx.connected_components(G), key=len)\n",
    "    \n",
    "    # Sort the values in the largest component to get the interval\n",
    "    largest_interval = sorted(largest_component)\n",
    "    \n",
    "    return largest_interval\n",
    "\n",
    "# Sample data based on the given example\n",
    "data = non_significant_pairs[['A', 'B', 'pval']]\n",
    "non_significant_pairs = pd.DataFrame(data)\n",
    "\n",
    "# Find the largest interval\n",
    "largest_interval = largest_non_significant_interval(non_significant_pairs)\n",
    "print(\"Largest interval of values that are not statistically different:\", largest_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
