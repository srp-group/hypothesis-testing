{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Uni\\WiSe 2024\\ML LAB\\ml_lab_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, Dataset\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch import nn, optim\n",
    "from torcheval import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "n_features = 180\n",
    "budget = 200\n",
    "n_classes = 3\n",
    "def read_file(filename, dataset):\n",
    "\n",
    "    # Read the file\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(' ')\n",
    "        parts = list(filter(lambda x: x.strip(), parts)) # Filter the values if there are empty spaces present\n",
    "        label = int(parts[0])\n",
    "        features = [0] * n_features # Initialise the features list according to the number of n_features\n",
    "        for part in parts[1:]:\n",
    "            index, value = part.split(':')\n",
    "            features[int(index) - 1] = float(value)  # Subtracting 1 to adjust 1-based to 0-based indexing\n",
    "\n",
    "        data.append([label] + features) # Add the label and feature in a line\n",
    "\n",
    "    # Create column names for DataFrame\n",
    "    columns = ['label'] + [f'feature_{i}' for i in range(1, n_features + 1)]\n",
    "\n",
    "    # Create a DataFrame from the list of lists and return it\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "dna_train = read_file('../data/dna.tr' , 'DNA')\n",
    "dna_test = read_file('../data/dna.val', 'DNA')\n",
    "dna_val = read_file('../data/dna.t', 'DNA')\n",
    "dna_train_val = pd.concat([dna_train, dna_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dna(Dataset):\n",
    "    dataset_name = \"dna\"\n",
    "    feature_encoder =  FunctionTransformer(lambda x: x)\n",
    "    target_encoder = OneHotEncoder(sparse_output=False)\n",
    "    urls_dict = {\"train\":\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/dna.scale.tr\",\n",
    "                \"val\":\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/dna.scale.val\",\n",
    "                \"test\":\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/dna.scale.t\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.location = \"datasets/data/\" + self.dataset_name\n",
    "        \n",
    "    def split(self, data):\n",
    "        data[\"train\"][\"x\"] = np.concatenate((data[\"train\"][\"x\"], data[\"val\"][\"x\"].copy()))\n",
    "        data[\"train\"][\"y\"] = np.concatenate((data[\"train\"][\"y\"], data[\"val\"][\"y\"].copy()))\n",
    "\n",
    "        data.pop(\"val\", None)\n",
    "        return data\n",
    "    def obtain(self):\n",
    "        data = {}\n",
    "        for split_name, url in self.urls_dict.items():\n",
    "            file_path = f\"{self.location}_{split_name}_raw\"\n",
    "            with open(file_path, 'w') as f:\n",
    "                r = requests.get(url)\n",
    "                f.writelines(r.content.decode(\"utf-8\"))\n",
    "            x, y  = load_svmlight_file(file_path, n_features=self.configs[\"n_features\"])\n",
    "            data[split_name] = {\"x\": np.asarray(x.todense(), dtype=np.float32), \"y\": y.reshape(-1, 1)}\n",
    "            os.remove(file_path)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\n\u001b[0;32m      3\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: dna_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: dna_test }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dict = {'train': dna_train, 'test': dna_test }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pool\n",
    "class Pool:\n",
    "    def __init__(self, data, random_seed = 42, val_share=0.25, n_initially_labeled=1000, batch_size=32):\n",
    "        self.random_seed = random_seed\n",
    "        self.set_seed(self.random_seed)\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.idx_abs = np.arange(len(self.data[\"train\"].x)) # Absolute index attribute\n",
    "        self.val_share = val_share\n",
    "        self.n_initially_labeled = n_initially_labeled\n",
    "        self.set_seed(self.random_seed)\n",
    "        self.idx_unviolated_lb = np.random.choice(self.idx_abs, size=self.n_initially_labeled, replace=False)\n",
    "        self.idx_new_lb = np.array([], dtype=int)\n",
    "        self.set_seed(self.random_seed)\n",
    "        self.test_loader = DataLoader(data[\"test\"], batch_size=self.batch_size, shuffle=False)\n",
    "        self.set_seed(self.random_seed)\n",
    "        self.splitter = ShuffleSplit(n_splits=6, \n",
    "                        test_size=self.val_share,\n",
    "                        random_state=self.random_seed)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[\"train\"][idx]\n",
    "        \n",
    "        @property\n",
    "        def drop_last(self):\n",
    "            # drop last if the number of labeled instances is bigger than the batch_size\n",
    "            return int(self.get_len(\"unviolated\")*(1-self.val_share)) + self.get_len(\"new_labeled\") > self.batch_size \n",
    "\n",
    "        @property\n",
    "        def idx_all_labeled(self):\n",
    "            return np.append(self.idx_unviolated_lb, self.idx_new_lb)\n",
    "        \n",
    "        @property\n",
    "        def new_lb_dataset(self):\n",
    "            return Subset(self.data['train'], self.idx_new_lb)\n",
    "        \n",
    "        @property\n",
    "        def unviolated_lb_dataset(self):\n",
    "            return Subset(self.data['train'], self.idx_unviolated_lb)\n",
    "        \n",
    "        @property\n",
    "        def all_lb_dataset(self):\n",
    "            return Subset(self.data['train'], self.idx_all_labeled)\n",
    "        \n",
    "        @property\n",
    "        def idx_ulb(self):\n",
    "            return np.delete(self.idx_abs, self.idx_all_labeled) \n",
    "        \n",
    "        def one_split(self):\n",
    "            self.set_seed(seed=self.random_seed)\n",
    "            return next(self.splitter.split(self.unviolated_lb_dataset))\n",
    "        \n",
    "        def CV_splits(self):\n",
    "            self.set_seed(seed=self.const_seed)\n",
    "            return self.splitter.split(self.unviolated_lb_dataset)\n",
    "        def get_train_val_loaders(self, unviolated_train_idx, unviolated_val_idx):\n",
    "            unviolated_train_ds = Subset(self.unviolated_lb_dataset, unviolated_train_idx)\n",
    "            unviolated_val_ds = Subset(self.unviolated_lb_dataset, unviolated_val_idx)\n",
    "\n",
    "            self.set_seed(seed=self.random_seed)\n",
    "            train_loader = DataLoader(ConcatDataset((unviolated_train_ds, self.new_lb_dataset)),\n",
    "                                    batch_size=self.batch_size, \n",
    "                                    drop_last=self.drop_last,\n",
    "                                    shuffle=True)\n",
    "            \n",
    "            val_loader = DataLoader(unviolated_val_ds, batch_size=self.batch_size, shuffle=False)\n",
    "            return train_loader, val_loader\n",
    "\n",
    "        def get_len(self, pool=\"total\"):\n",
    "            return len(self.get(pool)[0])\n",
    "        \n",
    "        def add_new_inst(self, idx):\n",
    "            assert len(self.idx_ulb)\n",
    "            self.idx_new_lb = np.append(self.idx_new_lb, idx)\n",
    "\n",
    "        def get(self, pool):\n",
    "            if pool == \"all_labeled\":\n",
    "                return self[self.idx_all_labeled]\n",
    "            elif pool == \"unviolated\":\n",
    "                return self[self.idx_unviolated_lb] \n",
    "            elif pool == \"new_labeled\":\n",
    "                return self[self.idx_new_lb] \n",
    "            elif pool == \"unlabeled\":\n",
    "                return self[self.idx_ulb] \n",
    "            elif pool == \"total\":\n",
    "                return self[:]\n",
    "            elif pool == \"test\":\n",
    "                return self.data[\"test\"][:]\n",
    "            else:\n",
    "                raise NameError(\"There is no such name in the pool\")\n",
    "            \n",
    "        def set_seed(self, seed=None):\n",
    "            seed = self.random_seed\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            np.random.seed(seed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The Acquisition function\n",
    "class Random():\n",
    "    def __init__(self, \n",
    "                clf,\n",
    "                pool,\n",
    "                random_seed = 42,\n",
    "                budget = budget):\n",
    "        self.clf = clf       \n",
    "        self.pool = pool\n",
    "        self.random_seed = random_seed\n",
    "        self.budget = budget\n",
    "    def get_scores(self, values=None):\n",
    "        if values is None:\n",
    "            values = self.pool.get_len(\"unlabeled\")\n",
    "        else:\n",
    "            values = values[:, 0].ravel().shape[0]\n",
    "        return np.random.random(values)\n",
    "    def query(self):\n",
    "        all_scores = self.get_scores()\n",
    "        max_scores = np.argwhere(np.isclose(all_scores, all_scores.max())).ravel()            \n",
    "        self.pool.set_seed(self.random_seed)\n",
    "        idx = np.random.choice(max_scores, 1)[0]\n",
    "        return self.pool.idx_ulb[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        self.layers.add_module(f\"dense_0\", nn.Linear(n_features, n_features//5))\n",
    "        self.layers.add_module(f\"activation_0\", nn.ReLU())\n",
    "        self.layers.add_module(f\"dense_1\", nn.Linear(n_features//5, n_features//10))\n",
    "        self.layers.add_module(f\"activation_1\", nn.ReLU())\n",
    "        self.layers.add_module(f\"dense_2\", nn.Linear(n_features//10, n_classes))\n",
    "        self.layers.add_module(f\"activation_2\", nn.Softmax())\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, weight_decay=0.01)\n",
    "        self.metric = metrics.MulticlassAccuracy()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y_true):\n",
    "        return self.metric(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learnable():\n",
    "    def __init__(self, \n",
    "                pool,\n",
    "                random_seed = 42,\n",
    "                n_warmup_epochs=100,\n",
    "                patience=20,\n",
    "                epochs=200):\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.n_warmup_epochs = n_warmup_epochs\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.pool = pool\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = NN()   \n",
    "        \n",
    "    def __call__(self, x, mc_dropout=False):\n",
    "        if mc_dropout:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.model(x.to(self.device))\n",
    "    \n",
    "    def train_model(self):\n",
    "        unviolated_train_idx, unviolated_val_idx = self.pool.one_split()\n",
    "        train_loader, val_loader = self.pool.get_train_val_loaders(unviolated_train_idx, unviolated_val_idx)\n",
    "        train_perf, val_perf = self.fit(train_loader=train_loader, val_loader=val_loader)\n",
    "        test_perf, _ = self.eval(loader=self.pool.test_loader)\n",
    "        return train_perf, val_perf, test_perf\n",
    "\n",
    "    def eval(self, loader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                targets = targets.to(self.device)\n",
    "                inputs = inputs.to(self.device)\n",
    "                predictions = self(inputs.float())\n",
    "                batch_loss = self.model.criterion(predictions, targets)\n",
    "                total_loss += batch_loss.item()\n",
    "                acc = self.model.calculate_accuracy(predictions, targets)  \n",
    "        return total_loss, acc\n",
    "    \n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.model.train()        \n",
    "        for epoch_num in range(self.epochs):\n",
    "            for inputs, targets in train_loader:\n",
    "                targets = targets.to(self.device)\n",
    "                inputs = inputs.to(self.device)\n",
    "                predictions = self.model(inputs.float())\n",
    "                batch_loss = self.model.criterion(predictions, targets.float())\n",
    "                train_loss += batch_loss.item()\n",
    "                self.model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.model.optimizer.step()\n",
    "\n",
    "            train_loss, train_metrics = self.eval(loader=train_loader)\n",
    "            val_loss, val_metrics = self.eval(val_loader)\n",
    "        return (train_loss, train_metrics),  (val_loss, val_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearning():\n",
    "    def __init__(self, data_dict):\n",
    "        self.budget = budget\n",
    "        self.random_seed = 42\n",
    "        self.pool = Pool(data=data_dict)\n",
    "        self.clf = Learnable(pool=self.pool)\n",
    "        self.acq = Random(clf=self.clf, pool=self.pool)\n",
    "    \n",
    "    def show_intermediate_results(self, abs_idx, train_perf, val_perf, test_perf):\n",
    "        print(f'{abs_idx} {self.pool.get_len(\"all_labeled\")} {self.pool.get_len(\"unlabeled\")}\\n{train_perf}\\n{val_perf}\\n{test_perf}')\n",
    "    \n",
    "    def train_first_hypers(self):\n",
    "        train_perf, val_perf, test_perf = self.clf.train_model()\n",
    "        print(f\"Initial {train_perf}, {val_perf}, {test_perf}\")\n",
    "        return train_perf, val_perf, test_perf\n",
    "    \n",
    "    def run(self):\n",
    "        abs_idx = None\n",
    "        train_perf, val_perf, test_perf = self.train_first_hypers()\n",
    "        for iteration in range(0, self.budget):\n",
    "            abs_idx = self.acq.query()\n",
    "            self.pool.add_new_inst(abs_idx)\n",
    "            train_perf, val_perf, test_perf = self.clf.train_model()\n",
    "        print(f\"final {train_perf}, {val_perf}, {test_perf}, {abs_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActiveLearning().run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
